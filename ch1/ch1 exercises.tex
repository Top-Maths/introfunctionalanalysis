\documentclass[12pt]{article}
\usepackage{newtxtext}
\usepackage[margin=0.5in]{geometry}
\usepackage{amsmath, amssymb, amsthm, physics}
\theoremstyle{definition}
\usepackage{datetime}
\title{Introductory Functional Analysis \\ Chapter 1 Exercises}

\date{Last Updated: \today}
\author{Top Maths}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{observation}{Observation}


\begin{document}
	\maketitle 
	
	\noindent \textbf{1.1.} Prove\footnote{There is a typo in the book. The reverse triangle inequality is as I stated here.} the reverse triangle inequality: For vectors $x$ and $y$ in any normed linear space, $$\norm{x - y} \geq \abs{\norm{x} - \norm{y}}.$$
		\begin{proof}
			We have,
				\begin{align*}
					\norm{x} = \norm{x - y + y} \leq \norm{x - y} + \norm{y} \implies \norm{x} - \norm{y} \leq \norm{x - y}
				\end{align*}
			Note that this argument is reflexive in $x$ and $y$, we similarly obtain $$\norm{y} - \norm{x} \leq \norm{x - y}.$$ Putting both together, we get $$- \norm{x - y} \leq \norm{x} - \norm{y} \leq \norm{x - y}.$$ That is, $$|\norm{x} - \norm{y}| \leq \norm{x - y}$$ as desired. 
		\end{proof}
	
	\newpage 
	\noindent \textbf{1.2.} Show that $C[0, 1]$ is a Banach space in the supremum norm. Hint: if $\{f_n\}$ is a Cauchy sequence in $C[0, 1]$, then for each fixed $x \in [0, 1]$, $\{f_n(x)\}$ is a Cauchy sequence in $\mathbb{C}$, which is complete.
		\begin{proof}
			Suppose $\{f_n\}$ is a Cauchy sequence in $C[0, 1]$. That means, for all $\varepsilon > 0$, there exists $N \in \mathbb{N}$ such that for $n, m \geq N$ we have $$\norm{f_n - f_m} = \sup_{x \in [0, 1]}|f_n(x) - f_m(x)| < \varepsilon.$$ We also get that, for $m, n \geq N$, for all $a \in [0, 1]$,  \begin{equation} |f_n(a) - f_m(a)| \leq \sup_{x \in [0, 1]} |f_n(x) - f_m(x)| < \varepsilon.\end{equation} In particular, since $\{f_n\}$ is a Cauchy sequence in $C[0, 1]$, $\{f_n(a)\}$ is a Cauchy sequence in $\mathbb{C}$ for all $a \in [0, 1]$. Since $\mathbb{C}$ is complete, $\{f_n(a)\}$ converges (because it is Cauchy) for all $a \in [0, 1]$. For each $a \in [0, 1]$, define $$f(a) := \lim_{n \to \infty} f_n(a).$$ Considering (1) again , we have by the continuity of $|\cdot|$,  $$\lim_{m \to \infty} |f_n(a) - f_m(a)| = |f_n(a) - \lim_{m \to \infty} f_m(a)| = |f_n(a) - f(a)| < \varepsilon.$$ Since this $\varepsilon$ is independent of our choice of $a \in [0, 1]$, it follows that $$\sup |f_n(a) - f(a)| = \norm{f_n - f} < \varepsilon$$ giving that $f_n \to f$ in $C[0, 1]$. To establish the continuity of $f$, pick $a \in [0, 1]$ and let $\varepsilon > 0$ arbitrary. Since each $f_n$ is continuous, there exists $\delta > 0$ such that $$|x - a| < \delta \implies |f_n(x) - f_n(a)| < \frac{\varepsilon}{3}.$$ Also, pick $n$ large enough so that  $$\sup_{p \in [0, 1]}|f_n(p) - f(p)| < \frac{\varepsilon}{3}.$$ Then,
				\begin{align*}
					|f(x) - f(a)| &= |f(x) - f(a) + f_n(x) - f_n(x) + f_n(a) - f_n(a)| \\
					&= |f(x) - f_n(x) + f_n(x) - f_n(a) +f_n(a) - f(a)| \\
					&\leq |f(x) - f_n(x)| + \underbrace{|f_n(x) - f_n(a)|}_{< \varepsilon/3} + |f_n(a) - f(a)| \\
					&\leq \norm{f - f_n} + \frac{\varepsilon}{3} + \norm{f_n - f} \\
					&\leq \frac{\varepsilon}{3} + \frac{\varepsilon}{3} + \frac{\varepsilon}{3} = \varepsilon.
				\end{align*}
		\end{proof} 
	\newpage 
	\noindent \textbf{1.3.} Let $C^1[0, 1]$ be the space of continuous, complex-valued functions on $[0, 1]$ with continuous first derivative. Show that with the supremum norm $\norm{\cdot}_\infty$, $C^1[0, 1]$ is not a Banach space, but that in the norm defined by $\norm{f} = \norm{f}_\infty + \norm{f'}_\infty$ it does become a Banach space. 
		\begin{proof}
			Let $\{f_n\}_{n \in \mathbb{N}} \subseteq C^1[0, 1]$ be defined by $$f_n(x) = \sqrt{x + \frac{1}{n}} \qquad (x \in [0, 1]).$$ Observe that $\sqrt{x}$ is uniformly continuous\footnote{Continuous functions on compact sets, such as $[0, 1]$ in question here, are always uniformly continuous} on $[0, 1]$, and so for all $\varepsilon > 0$ there exists $\delta > 0$ such that for any two points $p_1, p_2 \in [0, 1]$, $$|p_1 - p_2| < \delta \implies |f(p_1) - f(p_2)| < \varepsilon.$$  Take $p_1 = x + 1/n$ and $p_2 = x$, then the uniform continuity condition yields $$\frac{1}{n} < \delta \implies \abs{\sqrt{x + 1/n} - \sqrt{x}} < \varepsilon.$$ There exists $N \in \mathbb{N}$ large enough so that $1/n < \delta$, and so there exists $n \in \mathbb{N}$ large enough so that $$\abs{\sqrt{x + 1/n} - \sqrt{x}} < \varepsilon.$$ Since this $\varepsilon$ comes from the uniform continuity condition, it is independent of the choice of $x$, as is $N$, and therefore there exists $N \in \mathbb{N}$ such that for $n \geq N$, $$\sup_{x \in [0, 1]} \abs{\sqrt{x + 1/n} - \sqrt{x}} < \varepsilon.$$ While $\sqrt{x + 1/n}$ converges to $\sqrt{x}$ in the supremum norm, we find that $\sqrt{x} \notin C^1[0, 1]$ because the derivative of $\sqrt{x}$: $$\frac{d}{dx} \sqrt{x} = \frac{1}{2 \sqrt{x}}$$ is not differentiable at 0, and hence $(C^1[0, 1], \norm{\cdot}_\infty)$ is not a Banach space. 
			
			
			
			We'll consider the new norm $\norm{\cdot}$ as defined above. Suppose $\{f_n\}_{n \in \mathbb{N}}$ is a Cauchy sequence in $(C^1[0, 1], \norm{\cdot})$. And so for all $\varepsilon > 0$, there exists $N \in \mathbb{N}$ such that for $m,n \geq N$, $$\norm{f_n - f_m} = \norm{f_n - f_m}_\infty + \norm{f_n' - f_m'}_\infty < \varepsilon.$$ It follows immediately that $\{f_n\}_{n \in \mathbb{N}}$ and $\{f_n'\}_{n \in \mathbb{N}}$ are a Cauchy sequences in $C[0,1]$, and from our previous work we know $\lim_{n \to \infty} f_n = f$ and $\lim_{n \to \infty} f_n' = g$, where $$f(x) := \lim_{n \to \infty} f_n(x), \qquad g(x) := \lim_{n \to \infty} f_n'(x)$$ for all $x \in [0, 1]$. In fact, since this convergence is in the sup-norm, it follows\footnote{See \textit{Principles of Mathematical Analysis} (PMA) by Walter Rudin, 3rd Edition, Theorem 7.9 on p. 148} that $\{f_n\}_{n \in \mathbb{N}}$ uniformly converges to $f$ and $\{f_n'\}_{n \in \mathbb{N}}$ uniformly converges to $g$. But, we now have the conditions acquired\footnote{See PMA, Theorem 7.17, p. 152} to say that $$g(x) = f'(x) = \lim_{n \to \infty} f'(x).$$ Therefore, $$\lim_{n \to \infty} \norm{f_n - f} = \lim_{n \to \infty} \norm{f_n - f}_\infty + \lim_{n \to \infty} \norm{f_n' - f'}_\infty = 0.$$ Hence, $C^1[0,1]$ is complete under this new norm. 
		\end{proof}
	
	\newpage 
	\noindent \textbf{1.4.} Show that the space $\ell^1$ of Example 1.5 is complete. 
		\begin{proof}
			Writing down the definition of $\ell^1$ for convenience, we have:
				$$\ell^1 = \left\{\{a_n\}_{n \in \mathbb{N}} \subseteq \mathbb{C} : \norm{\{a_n\}}_1 = \sum_{n = 1}^\infty |a_n| < \infty \right\}.$$ 
			Let $\{\{a_m^n\}_{m \in \mathbb{N}}\}_{n \in \mathbb{N}}$ be a notations nightmare, but also let it be a Cauchy sequence in $\ell^1$, and so for all $\varepsilon > 0$ there exists $N \in \mathbb{N}$ such that for $k, \ell \geq N$, $$\norm{\{a_n^\ell\} - \{a_n^k\}} =  \norm{\{a_n^\ell - a_n^k\}} = \sum_{n = 1}^\infty |a_n^\ell - a_n^k| < \varepsilon.$$ For a fixed $n^\ast$, we have $$|a_{n^\ast}^\ell - a_{n^\ast}^k| < \sum_{n = 1}^\infty |a_n^\ell - a_n^k| < \varepsilon$$ and so, for each fixed $n^\ast \in \mathbb{N}$, $$\{a_{n^\ast}^\ell\}_{\ell \in \mathbb{N} }$$ is a Cauchy sequence in $\mathbb{C}$. For all $n \in \mathbb{N}$, let $\{a_n\}_{n \in \mathbb{N}}$ be the sequence defined by $$a_n = \lim_{\ell \to \infty} a_n^\ell$$ for all $n \in \mathbb{N}.$ First, we check that $\sum |a_n| < \infty$. We have,
				\begin{align*}
					\sum_{n = 1}^\infty |a_n| &= \lim_{\ell \to \infty}\sum_{n = 1}^\infty |a_n^\ell| 
				\end{align*} 
			Is this limit finite? In terms of the reverse triangle inequality for norms, we have, $$\abs{\sum_{n = 1}^\infty |a_n^\ell| - \sum_{n = 1}^\infty |a_n^k|} \leq \sum_{n = 1}^\infty |a_n^\ell - a_n^k| < \infty$$ by our Cauchy condition we obtained earlier. This result gives that the sequence of sums $$\left\{\sum_{n = 1}^\infty |a_n^\ell|\right\}_{\ell \in \mathbb{N}}$$ is a Cauchy sequence of complex numbers, and therefore converges. Hence, $\{a_n\}_{n \in \mathbb{N}} \in \ell^1$. Now, to prove $\lim_{\ell \to \infty} \{a_n^\ell\}_{n \in \mathbb{N}} = \{a_n\}_{n \in \mathbb{N}}$, we have
				\begin{align*}
					\lim_{\ell \to \infty} \norm{\{a_n^\ell\} - \{a_n\}}_1 &= \sum_{n = 1}^\infty |a_n^\ell - a_n| \\
					&= \sum_{n = 1}^\infty |\lim_{\ell \to \infty} a_n^\ell - a_n| \\
					&= 0.
				\end{align*}
			We have shown $\ell^1$ is complete, as desired. 
		\end{proof} 

\noindent \textbf{1.5.} Show that a metric space is complete if every Cauchy sequence has a convergent subsequence. 
	\begin{proof}
		Let $(X, d)$ be a metric space having the property that every Cauchy sequence in $X$ has a convergent subsequence. Given any sequence $\{x_n\}_{n \in \mathbb{N}}$ in $X$, let $\{x_{n_j}\}_{j \in \mathbb{N}}$ be a convergent subsequence to $x \in X$. Since $\{x_n\}_{n \in \mathbb{N}}$ is Cauchy, there exists $N_1 \in \mathbb{N}$ such that for $j, m \geq N_1$, $$d(x_m, x_{n_j}) < \frac{\varepsilon}{2}.$$ Moreover, by the fact $\lim_{j \to \infty} x_{n_j} = x$, there exists $N_2 \in \mathbb{N}$ such that for $j \geq N_2$, $$|x_{n_j} - x| < \frac{\varepsilon}{2}.$$ Take $N = \max\{N_1, N_2\}$, and therefore for $n, j \geq N$, 
		 $$d(x_n, x) \leq d(x_n, x_{n_j}) + d(x_{n_j}, x) < \frac{\varepsilon}{2} + \frac{\varepsilon}{2} = \varepsilon.$$ Hence, $\lim_{n \to \infty} x_n = x$ and we conclude that $(X, d)$ is complete.
	\end{proof} 
	
\noindent \textbf{1.6.} Assume that you now Minkowski's inequality $$\left(\sum_{j = 1}^n |a_j + b_j|^2\right)^{1/2} \leq \left(\sum_{j = 1}^n |a_j|^2\right)^{1/2} + \left(\sum_{j = 1}^n |b_j|^2\right)^{1/2}$$ for $\mathbb{C}^n$ in the Euclidean norm. 
	\begin{itemize}
		\item[(a)] Show that for $\{a_n\}$ and $\{b_n\}$ in a weighted sequence space $\ell_\beta^2$, $$\left(\sum_{j = 0}^\infty |a_j + b_j|^2 \beta(j)^2\right)^{1/2} \leq \left(\sum_{j = 0}^\infty |a_j|^2 \beta(j)^2 \right)^{1/2} + \left(\sum_{j = 0}^\infty |b_j|^2 \beta(j)^2\right)^{1/2}$$ 
		
		\item[(b)] Verify directly that $\ell_\beta^2$ is complete. 
	\end{itemize}
	
	\begin{proof} ~
		\begin{itemize}
			\item[(a)] For convenience, we recall the definition of $\ell_\beta^2$. For $\{\beta(n)\}_{n = 0}^\infty$ a sequence of positive numbers with $\beta(0) = 1$, $\lim_{n \to \infty} \beta(n)^{1/n} \geq 1$, we have $\{a_n\}_{n = 0}^\infty \in \ell_\beta^2$ if and only if $$\sum_{n = 0}^\infty |a_n|^2 \beta(n)^2 < \infty.$$ 
			Suppose that $\{a_n\}, \{b_n\} \in \ell_\beta^2$ and define $\{a_n'\}$, $\{\beta_n'\}$ by setting $a_j' = a_j \beta(j)$, $b_j' = b_j \beta(j)$. For all $m \in \mathbb{N}$, we have by the given finite Minkowski inequality over $\mathbb{C}^m$: $$\left(\sum_{j = 1}^m |a_j' + b_j'|^2\right)^{1/2} \leq \left(\sum_{j = 1}^m |a_j'|^2\right)^{1/2} + \left(\sum_{j = 1}^m |b_j'|^2\right)^{1/2}.$$ Replacing $a_j', b_j'$ in terms of $a_j$ and $b_j$, we obtain the inequality $$\left(\sum_{j = 1}^m |a_j + b_j|^2\beta(j)^2\right)^{1/2} \leq \left(\sum_{j = 1}^m |a_j|^2 \beta(j)^2\right)^{1/2} + \left(\sum_{j = 1}^m |b_j|^2 \beta(j)^2\right)^{1/2}$$ for all $m \in \mathbb{N}$. Given that $\{a_n\}$, $\{b_n\}$ are in $\ell_\beta^2$, the right hand side converges as $m \to \infty$, and therefore does the left hand side. Thus, the inequality is preserved taking limits as $m \to \infty$, and we obtain the desired weighted Minkowski inequality.
			
			\item[(b)] It follows from (a) that the weighted norm is indeed a norm over $\ell_\beta^2$. Now, let $\{\{a_m^n\}_{m \in \mathbb{N}}\}_{n \in \mathbb{N}}$ be a Cauchy sequence in $\ell_\beta^2$, and so for all $\varepsilon > 0$ there exists $N \in \mathbb{N}$ such that for $k, l \geq N$, $$\norm{\{a_n^l\} - \{a_n^k\}} = \norm{\{a_n^l - a_n^k\}} = \left(\sum_{n = 0}^\infty |a_n^l - a_n^k|^2 \beta(n)^2\right)^{1/2} < \varepsilon.$$ For a fixed $n^\ast$, we have $$|a_{n^\ast}^l - a_{n^\ast}^k|\beta(n^\ast) \leq \left(\sum_{n = 0}^\infty |a_n^l - a_n^k|^2 \beta(n)^2\right)^{1/2} < \infty$$ and it follows that (taking $\varepsilon' = \varepsilon/\beta(n^\ast)$), for each $n^\ast \in \mathbb{N}$, $$\{a_{n^\ast}^l\}_{l \in \mathbb{N}}$$ is a Cauchy sequence in $\mathbb{C}$. For all $n \in \mathbb{N}$, let $\{a_n\}_{n \in \mathbb{N}}$ be the sequence defined by $$a_n = \lim_{l \to \infty} a_n^l$$ for all $n \in \mathbb{N}$. First, we check that $\sum |a_n|^2 \beta(n)^2 < \infty$. We have $$\sum_{n = 0}^\infty |a_n|^2 \beta(n)^2 = \lim_{l \to \infty} \sum_{n = 1}^\infty |a_n^l|^2 \beta(n)^2.$$ By the reverse triangle inequality for norms, we have $$\abs{\sum_{n = 0}^\infty |a_n^l|^2\beta(n)^2 - \sum_{n = 1}^\infty |a_n^k|^2\beta(n)^2} \leq \sum_{n = 0}^\infty |a_n^l - a_n^k|^2 \beta(j)^2 < \infty$$ by our Cauchy condition that we obtained earlier. This result gives that the sequence of sums $$\left\{\sum_{n  = 0}^\infty |a_n^l|^2 \beta(n)\right\}_{l \in \mathbb{N}}$$ is a Cauchy sequence of complex number, and therefore converges. Hence, $\{a_n\}_{n \in \mathbb{N}} \in \ell_\beta^2$. Now, to prove $\lim_{l \to \infty} \{a_n^l\}_{n \in \mathbb{N}} = \{a_n\}_{n \in \mathbb{N}}$, see that
				\begin{align*}
					\lim_{l \to \infty} \norm{\{a_n^l\} - \{a_n\}} &= \lim_{l \to \infty}\left(\sum_{n = 0}^\infty|a_n^l - a_n|^2 \beta(n)^2\right)^{1/2} \\
					&= \left(\sum_{n = 0}^\infty |\lim_{l \to \infty}a_n^l - a_n|^2 \beta(n)^2\right)^{1/2} \\
					&= 0. 
				\end{align*} 
			It follows that $\ell_\beta^2$ is complete. 
		\end{itemize}
	\end{proof}
	
\noindent \textbf{1.7.} Let $x$ and $y$ be any two vectors in an inner product space and set $\lambda = \langle y, y \rangle$. Show that $$\lambda [\lambda \langle x, x\rangle - |\langle x, y \rangle|^2] = \langle \lambda x - \langle x, y\rangle y, \lambda x - \langle x, y \rangle y \rangle.$$ Use this to derive the Cauchy-Schwarz inequality and to determine when equality holds in the Cauchy-Schwarz inequality. 
	\begin{proof}
		By direct computation, we have,
			\begin{align*}
				\langle \lambda x - \langle x, y\rangle y, \lambda x - \langle x, y \rangle y \rangle &= \langle\lambda x, \lambda x - \langle x, y \rangle y\rangle - \langle \langle x, y \rangle y, \lambda x - \langle x, y \rangle y \rangle \\
				&= \lambda \langle x, \lambda x - \langle x, y\rangle y \rangle - \langle x, y \rangle \langle y, \lambda x - \langle x, y\rangle y \rangle \\
				&= \lambda \overline{\langle \lambda x - \langle x, y \rangle y, x\rangle} - \langle x, y \rangle \overline{\langle \lambda x - \langle x, y \rangle y, y\rangle} \\
				&= \lambda (\overline{\lambda\langle  x, x \rangle - \langle x, y\rangle \langle y, x\rangle}) - \langle x, y \rangle (\overline{\lambda \langle x, y\rangle - \langle x, y\rangle \langle y, y \rangle}) \\
				&= \lambda (\overline{\lambda} \langle x, x\rangle - |\langle x, y \rangle|^2) - \langle x, y \rangle (\lambda\langle y, x \rangle - \lambda \langle y, x\rangle ) \\
				&= \lambda^2 \langle x, x\rangle - \lambda |\langle x, y\rangle|^2 - \lambda |\langle x, y \rangle|^2 + \lambda |\langle x, y \rangle|^2 \\
				&= \lambda^2 \langle x, x\rangle - \lambda |\langle x, y \rangle|^2 \\
				&= \lambda [\lambda\langle x, x \rangle - |\langle x, y \rangle|^2].
			\end{align*}
		Now, to derive the Cauchy-Schwarz inequality: Suppose $x, y \neq 0$, otherwise the inequality holds trivially. Note that by Definition 1.13(2), 
			\begin{align*}
				\langle \lambda x - \langle x, y \rangle y, \lambda x - \langle x, y \rangle y \rangle \geq 0 &\implies \lambda [\lambda \langle x, x \rangle - |\langle x, y \rangle|^2] \geq 0 \\
				&\implies \lambda \langle x, x \rangle - |\langle x, y \rangle|^2 \geq 0\\ 
				&\implies \lambda \langle x, x \rangle \geq |\langle x, y \rangle|^2 \\
				&\implies \langle x, x \rangle \langle y, y \rangle \leq |\langle x, y \rangle|^2
			\end{align*}
		giving the result. Now, if $|\langle x, y \rangle|^2 = \langle x, x \rangle \langle y, y \rangle$, it follows that by the formula we've derived $$\langle \lambda x - \langle x, y \rangle y, \lambda x - \langle x, y \rangle y \rangle = 0.$$ By Definition 1.13(2), this implies that $\lambda x - \langle x, y \rangle y = 0$, or rather that $$x = \frac{\langle x, y \rangle}{\langle y, y\rangle} y$$ so that $x$ is a scalar multiple of $y$. So, for equality to hold, $x$ must be a scalar multiple of $y$. To show that this is sufficient, let $x = cy$ for some $c \in \mathbb{C}$. Then, $$\langle y, y \rangle \langle x, x \rangle - |\langle x, y \rangle|^2 = |c|^2 \langle x, x\rangle^2 - |c|^2 \langle x, x \rangle^2 = 0.$$ 
	\end{proof}

\newpage 
\noindent \textbf{Problem 1.8.} ~
	\begin{itemize}
		\item[(a)] Show that for a normed linear space $X$, the map $x \mapsto \norm{x}$ of $X$ into $[0, \infty)$ is continuous. Is it uniformly continuous?
		
		\item[(b)] Show that the mappings $X \times X \to X$ given by $(x, y) \mapsto x + y$, and $\mathbb{C} \times X \to X$ given by $(\alpha, ) \mapsto \alpha x$ are continuous. The topologies on $X \times X$ and $\mathbb{C} \times X$ are the product topologies.  
		
		\item[(c)] Suppose that $X$ is an inner product space. Show that the maps $x \mapsto \langle x, y\rangle$ and $x \mapsto \langle y, x\rangle$ are continuous on $X$ for each fixed $y$ in $X$. Are they uniformly continuous? 
	\end{itemize}
	\begin{proof} ~
		\begin{itemize}
			\item[(a)]
			
			\item[(b)]
			
			\item[(c)] 
		\end{itemize}
	\end{proof}
\end{document}